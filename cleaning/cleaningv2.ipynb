{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8303fb7b",
   "metadata": {},
   "source": [
    "# Healthcare Data Cleaning Notebook\n",
    "## Introduction\n",
    "This notebook takes the raw (but previously cleaned) healthcare datasets and applies a series of cleaning and transformation steps based on our earlier analysis. The goal is to prepare the data for loading into a database or for further analysis. Each step is broken down into a separate cell for clarity.\n",
    "### Imports and Setup\n",
    "First, we import the necessary libraries. pandas is for data manipulation and os is for interacting with the file system (to create directories and manage file paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf825f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ed28be",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "Design Decision: Define input and output file paths as variables in a dedicated configuration cell.\n",
    "Why? This separates configuration from the core logic. Instead of hard-coding file paths deep inside the script, this design allows anyone (including your future self) to easily reuse this notebook for different data by only changing these two variables. It makes the code more maintainable and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "475ba96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.path.join('..', 'cleaned_data')\n",
    "OUTPUT_DATA_DIR = os.path.join('..', 'v2cleaned_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef3e1c",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed8695",
   "metadata": {},
   "source": [
    "Load all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c14904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    observations = pd.read_csv(os.path.join(INPUT_DATA_DIR, \"observations_cleaned.csv\"))\n",
    "    patients = pd.read_csv(os.path.join(INPUT_DATA_DIR, \"patients_cleaned.csv\"))\n",
    "    procedures = pd.read_csv(os.path.join(INPUT_DATA_DIR, \"procedures_cleaned.csv\"))\n",
    "    diagnoses = pd.read_csv(os.path.join(INPUT_DATA_DIR, \"diagnoses_cleaned.csv\"))\n",
    "    encounters = pd.read_csv(os.path.join(INPUT_DATA_DIR, \"encounters_cleaned.csv\"))\n",
    "    medications = pd.read_csv(os.path.join(INPUT_DATA_DIR, \"medications_cleaned.csv\"))\n",
    "    print(\"All datasets loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[ERROR] A data file was not found. Please check the input directory path. Details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7b18a",
   "metadata": {},
   "source": [
    "Lets double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b12597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>observation_code</th>\n",
       "      <th>observation_datetime</th>\n",
       "      <th>observation_description</th>\n",
       "      <th>observation_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>units</th>\n",
       "      <th>value_numeric</th>\n",
       "      <th>value_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f5f83a54-5883-413d-9bb4-c859fa6b8cde</td>\n",
       "      <td>4548-4</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>Hemoglobin A1c/Hemoglobin.total in Blood</td>\n",
       "      <td>c70dc224-4c15-43ec-89b6-ed7821d80df2</td>\n",
       "      <td>ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb</td>\n",
       "      <td>%</td>\n",
       "      <td>7.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f5f83a54-5883-413d-9bb4-c859fa6b8cde</td>\n",
       "      <td>2345-7</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>Glucose [Mass/Vol]</td>\n",
       "      <td>065df109-6962-496e-82a7-ab975746f265</td>\n",
       "      <td>ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb</td>\n",
       "      <td>mg/dL</td>\n",
       "      <td>210.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f5f83a54-5883-413d-9bb4-c859fa6b8cde</td>\n",
       "      <td>2160-0</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>Creatinine [Mass/Vol]</td>\n",
       "      <td>ea1a0317-d4cf-4f4c-9d3b-9e87700f67bc</td>\n",
       "      <td>ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb</td>\n",
       "      <td>mg/dL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4345130-e167-45b5-9e60-75a1815d3ae0</td>\n",
       "      <td>8480-6</td>\n",
       "      <td>2026-04-08</td>\n",
       "      <td>Systolic blood pressure</td>\n",
       "      <td>8cd3eab8-0a7b-4e49-856c-ba2a081f969f</td>\n",
       "      <td>ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb</td>\n",
       "      <td>mmHg</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a4345130-e167-45b5-9e60-75a1815d3ae0</td>\n",
       "      <td>8462-4</td>\n",
       "      <td>2026-04-08</td>\n",
       "      <td>Diastolic blood pressure</td>\n",
       "      <td>31cb9c28-2bad-431a-b59a-6be7750e3184</td>\n",
       "      <td>ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb</td>\n",
       "      <td>mmHg</td>\n",
       "      <td>68.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           encounter_id observation_code observation_datetime  \\\n",
       "0  f5f83a54-5883-413d-9bb4-c859fa6b8cde           4548-4           2025-04-14   \n",
       "1  f5f83a54-5883-413d-9bb4-c859fa6b8cde           2345-7           2025-04-14   \n",
       "2  f5f83a54-5883-413d-9bb4-c859fa6b8cde           2160-0           2025-04-14   \n",
       "3  a4345130-e167-45b5-9e60-75a1815d3ae0           8480-6           2026-04-08   \n",
       "4  a4345130-e167-45b5-9e60-75a1815d3ae0           8462-4           2026-04-08   \n",
       "\n",
       "                    observation_description  \\\n",
       "0  Hemoglobin A1c/Hemoglobin.total in Blood   \n",
       "1                        Glucose [Mass/Vol]   \n",
       "2                     Creatinine [Mass/Vol]   \n",
       "3                   Systolic blood pressure   \n",
       "4                  Diastolic blood pressure   \n",
       "\n",
       "                         observation_id                            patient_id  \\\n",
       "0  c70dc224-4c15-43ec-89b6-ed7821d80df2  ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb   \n",
       "1  065df109-6962-496e-82a7-ab975746f265  ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb   \n",
       "2  ea1a0317-d4cf-4f4c-9d3b-9e87700f67bc  ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb   \n",
       "3  8cd3eab8-0a7b-4e49-856c-ba2a081f969f  ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb   \n",
       "4  31cb9c28-2bad-431a-b59a-6be7750e3184  ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb   \n",
       "\n",
       "   units  value_numeric value_text  \n",
       "0      %            7.6        NaN  \n",
       "1  mg/dL          210.0        NaN  \n",
       "2  mg/dL            1.0        NaN  \n",
       "3   mmHg          101.0        NaN  \n",
       "4   mmHg           68.0        NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ef3ff",
   "metadata": {},
   "source": [
    "### Basic Data Type Cleaning (patients)\n",
    "Design Decisions:\n",
    "* Use pd.to_datetime with errors='coerce'.\n",
    "* Convert zip_code to a string type using .astype(str).\n",
    "#### Why?\n",
    "* pd.to_datetime: This is the standard pandas function for converting columns to a proper date format, which is essential for correct sorting and calculations. errors='coerce' is a crucial safety feature that will turn malformed dates into NaT (Not a Time) instead of crashing the script.\n",
    "* .astype(str) for zip codes: Zip codes are identifiers, not numbers meant for mathematical operations. Storing them as numbers can cause problems, like automatically removing leading zeros (e.g., 07960 would become 7960). Converting to a string preserves the exact format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2859f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   address        150 non-null    object        \n",
      " 1   city           150 non-null    object        \n",
      " 2   date_of_birth  150 non-null    datetime64[ns]\n",
      " 3   first_name     150 non-null    object        \n",
      " 4   gender         150 non-null    object        \n",
      " 5   last_name      150 non-null    object        \n",
      " 6   patient_id     150 non-null    object        \n",
      " 7   phone_number   150 non-null    object        \n",
      " 8   state          150 non-null    object        \n",
      " 9   zip_code       150 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(9)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "patients['date_of_birth'] = pd.to_datetime(patients['date_of_birth'], errors='coerce')\n",
    "patients['zip_code'] = patients['zip_code'].astype(str)\n",
    "patients.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86206cee",
   "metadata": {},
   "source": [
    "### Advanced Cleaning: Standardizing Text and Categorical Data (patients)\n",
    "Design Decisions:\n",
    "* Use .str.title() for names and cities to enforce proper case.\n",
    "* Use .str.upper() for state abbreviations.\n",
    "* Convert the gender column to the category data type.\n",
    "### Why?\n",
    "* Case Standardization (.str.title()/.str.upper()): Real-world data is messy. You might have \"new york\", \"New York\", and \"NEW YORK\" all referring to the same city. Standardizing the case makes the data consistent, which is essential for accurate grouping and analysis. We use title case for names/cities and upper case for state codes, as is standard convention.\n",
    "* Categorical Data (.astype('category')): For columns with a small, fixed number of unique values (like gender), converting to the category type is more memory-efficient than leaving it as object (string). It can also speed up some operations. This is a best practice for clean, optimized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ae1e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients['first_name'] = patients['first_name'].str.title()\n",
    "patients['last_name'] = patients['last_name'].str.title()\n",
    "patients['city'] = patients['city'].str.title()\n",
    "patients['state'] = patients['state'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8feb4f",
   "metadata": {},
   "source": [
    "Convert gender to a more efficient categorical type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eba30aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of cleaned patient names and locations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>first_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>last_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>state</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26236 Nunez Road Apt. 527</td>\n",
       "      <td>Sharpchester</td>\n",
       "      <td>1985-01-11</td>\n",
       "      <td>Juan</td>\n",
       "      <td>Male</td>\n",
       "      <td>Calderon</td>\n",
       "      <td>ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb</td>\n",
       "      <td>+4246662958x202</td>\n",
       "      <td>MD</td>\n",
       "      <td>9173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90829 Thomas Summit</td>\n",
       "      <td>East Christophermouth</td>\n",
       "      <td>1981-12-11</td>\n",
       "      <td>Paul</td>\n",
       "      <td>Male</td>\n",
       "      <td>Price</td>\n",
       "      <td>0eeb5541-d0b3-47fe-839c-a2227526b751</td>\n",
       "      <td>+15154233703</td>\n",
       "      <td>ND</td>\n",
       "      <td>62828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3939 Sarah Ridges</td>\n",
       "      <td>Jeffreyburgh</td>\n",
       "      <td>1950-12-17</td>\n",
       "      <td>Julie</td>\n",
       "      <td>Female</td>\n",
       "      <td>Brown</td>\n",
       "      <td>83f30300-2873-49f7-8fe4-06903a75db73</td>\n",
       "      <td>+0016517844153</td>\n",
       "      <td>NH</td>\n",
       "      <td>80694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5559 Walton Inlet</td>\n",
       "      <td>West Holly</td>\n",
       "      <td>1963-01-22</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>Female</td>\n",
       "      <td>Dillon</td>\n",
       "      <td>3a707a9a-00b9-40f1-90bf-1a4ff74fcb61</td>\n",
       "      <td>+0019383725030x5868</td>\n",
       "      <td>AK</td>\n",
       "      <td>30233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4609 Reginald Plaza Apt. 985</td>\n",
       "      <td>Megantown</td>\n",
       "      <td>1943-01-06</td>\n",
       "      <td>Laura</td>\n",
       "      <td>Female</td>\n",
       "      <td>Brown</td>\n",
       "      <td>825e3f21-ca2a-442a-8d95-7f3dd64c3c6a</td>\n",
       "      <td>+8447233702</td>\n",
       "      <td>FL</td>\n",
       "      <td>40222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        address                   city date_of_birth  \\\n",
       "0     26236 Nunez Road Apt. 527           Sharpchester    1985-01-11   \n",
       "1           90829 Thomas Summit  East Christophermouth    1981-12-11   \n",
       "2             3939 Sarah Ridges           Jeffreyburgh    1950-12-17   \n",
       "3             5559 Walton Inlet             West Holly    1963-01-22   \n",
       "4  4609 Reginald Plaza Apt. 985              Megantown    1943-01-06   \n",
       "\n",
       "  first_name  gender last_name                            patient_id  \\\n",
       "0       Juan    Male  Calderon  ea3a68f6-ecf9-46aa-be97-7ecbfc7e7fcb   \n",
       "1       Paul    Male     Price  0eeb5541-d0b3-47fe-839c-a2227526b751   \n",
       "2      Julie  Female     Brown  83f30300-2873-49f7-8fe4-06903a75db73   \n",
       "3      Sarah  Female    Dillon  3a707a9a-00b9-40f1-90bf-1a4ff74fcb61   \n",
       "4      Laura  Female     Brown  825e3f21-ca2a-442a-8d95-7f3dd64c3c6a   \n",
       "\n",
       "          phone_number state zip_code  \n",
       "0      +4246662958x202    MD     9173  \n",
       "1         +15154233703    ND    62828  \n",
       "2       +0016517844153    NH    80694  \n",
       "3  +0019383725030x5868    AK    30233  \n",
       "4          +8447233702    FL    40222  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients['gender'] = patients['gender'].astype('category')\n",
    "\n",
    "print(\"\\nSample of cleaned patient names and locations:\")\n",
    "patients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a05f0",
   "metadata": {},
   "source": [
    "### Feature Engineering: Creating an age Column (patients)\n",
    "Design Decision: Create a new age column by calculating the difference between a fixed current date and the patient's date of birth.\n",
    "Why? This is a form of feature engineeringâ€”creating a new, useful piece of information from existing data. An age column is much more useful for analysis than a date_of_birth column. For example, we can now easily analyze health trends by age group. We use a fixed date for reproducibility; using pd.Timestamp.now() would cause the age to change every time the script is run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d40dfc",
   "metadata": {},
   "source": [
    "Use a fixed date for reproducibility of the age calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fc7d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_date = pd.to_datetime('2025-06-27')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc13602",
   "metadata": {},
   "source": [
    "Calculate age in years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92e9f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'age' column created.\n",
      "\n",
      "Sample of patients with new 'age' column:\n",
      "  first_name last_name date_of_birth  age\n",
      "0       Juan  Calderon    1985-01-11   40\n",
      "1       Paul     Price    1981-12-11   43\n",
      "2      Julie     Brown    1950-12-17   74\n",
      "3      Sarah    Dillon    1963-01-22   62\n",
      "4      Laura     Brown    1943-01-06   82\n"
     ]
    }
   ],
   "source": [
    "patients['age'] = (analysis_date - patients['date_of_birth']).dt.days // 365\n",
    "print(\"'age' column created.\")\n",
    "print(\"\\nSample of patients with new 'age' column:\")\n",
    "print(patients[['first_name', 'last_name', 'date_of_birth', 'age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833da9d",
   "metadata": {},
   "source": [
    "### Data Validation and Feature Engineering (encounters)\n",
    "Design Decisions:\n",
    "* Validate that discharge_date is not before admission_date.\n",
    "* Create a new visit_duration_days column.\n",
    "Why?\n",
    "* Logical Validation: It's logically impossible for a patient to be discharged before they are admitted. This check ensures data integrity. We will flag these rows rather than deleting them, as they may require manual investigation.\n",
    "* Feature Engineering: visit_duration_days is a much more valuable feature for analysis than the two separate date columns. It allows us to quickly find the length of each visit. We calculate this only for valid date ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93839e69",
   "metadata": {},
   "source": [
    "First, convert dates to the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68d9d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters['admission_date'] = pd.to_datetime(encounters['admission_date'], errors='coerce')\n",
    "encounters['discharge_date'] = pd.to_datetime(encounters['discharge_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1ef23",
   "metadata": {},
   "source": [
    "Validation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a587ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encounter date logic is valid.\n"
     ]
    }
   ],
   "source": [
    "invalid_dates = encounters[encounters['discharge_date'] < encounters['admission_date']]\n",
    "if not invalid_dates.empty:\n",
    "    print(f\"[WARNING] Found {len(invalid_dates)} encounters with discharge date before admission date.\")\n",
    "else:\n",
    "    print(\"Encounter date logic is valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fe03c",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93648f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      admission_date                 discharge_date  \\\n",
      "count                            291                            291   \n",
      "mean   2011-07-02 12:51:57.525773312  2011-07-02 13:31:32.783505152   \n",
      "min              1970-09-02 00:00:00            1970-09-02 00:00:00   \n",
      "25%              2003-05-24 12:00:00            2003-05-24 12:00:00   \n",
      "50%              2015-11-09 00:00:00            2015-11-09 00:00:00   \n",
      "75%              2023-09-14 00:00:00            2023-09-14 00:00:00   \n",
      "max              2027-05-12 00:00:00            2027-05-12 00:00:00   \n",
      "std                              NaN                            NaN   \n",
      "\n",
      "       visit_duration_days  \n",
      "count           291.000000  \n",
      "mean              0.027491  \n",
      "min               0.000000  \n",
      "25%               0.000000  \n",
      "50%               0.000000  \n",
      "75%               0.000000  \n",
      "max               1.000000  \n",
      "std               0.163792  \n"
     ]
    }
   ],
   "source": [
    "encounters['visit_duration_days'] = (encounters['discharge_date'] - encounters['admission_date']).dt.days\n",
    "print(encounters[['admission_date', 'discharge_date', 'visit_duration_days']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea2ed9",
   "metadata": {},
   "source": [
    "### Advanced Cleaning and Outlier Detection (observations)\n",
    "Design Decisions:\n",
    "* Create a reusable function to detect outliers using the IQR method.\n",
    "* Apply this function to the value_numeric column.\n",
    "* Create a new boolean column is_outlier instead of removing the data.\n",
    "### Why?\n",
    "* IQR Method: The Interquartile Range (IQR) method is a standard and robust statistical technique for identifying outliers. It is less sensitive to a few extreme values than methods based on standard deviation.\n",
    "* Non-Destructive Flagging: In medical data, an extreme value might be a data entry error, or it could be a critical, life-threatening event. Deleting potential outliers is dangerous because it involves throwing away potentially vital information. Flagging them in a new column is a much safer approach. It preserves all original data while allowing analysts to easily include or exclude these values during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05108ee4",
   "metadata": {},
   "source": [
    "First, convert date to the correct type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a69e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations['observation_datetime'] = pd.to_datetime(observations['observation_datetime'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4251575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_outliers(df, column):\n",
    "    \"\"\"Flags outliers in a specified column of a DataFrame using the IQR method.\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (df[column] < lower_bound) | (df[column] > upper_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8428a",
   "metadata": {},
   "source": [
    "Apply the function to the numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "979fa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations['is_outlier'] = flag_outliers(observations, 'value_numeric')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c503025",
   "metadata": {},
   "source": [
    "Fill NaN in the new column with False, as missing values are not outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcf01892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged 9 potential outliers in 'value_numeric'.\n",
      "\n",
      "Sample of observations with outlier flag:\n",
      "      observation_description  value_numeric  is_outlier\n",
      "881   Systolic blood pressure         136.00       False\n",
      "882  Diastolic blood pressure          94.00       False\n",
      "883     Creatinine [Mass/Vol]           0.91       False\n",
      "884   Systolic blood pressure         132.00       False\n",
      "885  Diastolic blood pressure          84.00       False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erwin\\AppData\\Local\\Temp\\ipykernel_20496\\1633155868.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  observations['is_outlier'].fillna(False, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "observations['is_outlier'].fillna(False, inplace=True)\n",
    "\n",
    "outlier_count = observations['is_outlier'].sum()\n",
    "if outlier_count > 0:\n",
    "    print(f\"Flagged {outlier_count} potential outliers in 'value_numeric'.\")\n",
    "else:\n",
    "    print(\"No outliers detected in 'value_numeric'.\")\n",
    "\n",
    "print(\"\\nSample of observations with outlier flag:\")\n",
    "print(observations[['observation_description', 'value_numeric', 'is_outlier']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78514453",
   "metadata": {},
   "source": [
    "### Cleaning Remaining DataFrames (procedures, diagnoses, medications)\n",
    "Design Decision: Process the remaining tables, handling their specific data types and missing values as determined in our analysis.\n",
    "#### Why? \n",
    "Each table has unique characteristics. We apply the specific logic needed for each one: converting dates, and for medications, correctly handling missing dosage and end_date values to preserve their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebf0134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures['date_performed'] = pd.to_datetime(procedures['date_performed'], errors='coerce')\n",
    "diagnoses['date_recorded'] = pd.to_datetime(diagnoses['date_recorded'], errors='coerce')\n",
    "\n",
    "medications['start_date'] = pd.to_datetime(medications['start_date'], errors='coerce')\n",
    "medications['end_date'] = pd.to_datetime(medications['end_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d440031",
   "metadata": {},
   "source": [
    "Fill missing dosage with 'Unknown' - this is a data quality issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "281190e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [medication_order_id, patient_id, encounter_id, drug_code, drug_name, dosage, route, frequency, start_date, end_date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "medications['dosage'].fillna('Unknown', inplace=True)\n",
    "print(medications[medications['dosage'] == 'Unkown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98b633",
   "metadata": {},
   "source": [
    "### Preparing Data for PostgreSQL\n",
    "Design Decision: Save the final DataFrames as Parquet files instead of CSV.\n",
    "#### Why? This is a critical data engineering best practice.\n",
    "* CSV (The Alternative): CSV files are just plain text. They do not store metadata. When you save a DataFrame as a CSV, all data types are lost. 2025-06-27 becomes a simple string \"2025-06-27\", and the number 50 becomes the string \"50\". Your teammate would have to guess the correct data types when creating the PostgreSQL table, which often leads to errors.\n",
    "* Parquet (Our Choice): Parquet is a modern, columnar file format designed for efficiency and data-aware systems. It saves the schema (the column names and their data types) along with the data. When your teammate reads the Parquet file, it will know that age is an integer and admission_date is a timestamp. This makes the data handoff much more reliable and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938bb08",
   "metadata": {},
   "source": [
    "If you dont have pyarrow: pip install pyarrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94aad8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving cleaned dataframes to Parquet format...\n",
      "Successfully saved cleaned data to: ..\\v2cleaned_data\\medications.parquet\n",
      "\n",
      "Data cleaning process finished successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving cleaned dataframes to Parquet format...\")\n",
    "\n",
    "dataframes = {\n",
    "\"observations\": observations, \"patients\": patients, \"procedures\": procedures,\n",
    "\"diagnoses\": diagnoses, \"encounters\": encounters, \"medications\": medications\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "# Trim whitespace from all object (string) columns before saving\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].notna().any():\n",
    "            df[col] = df[col].str.strip()\n",
    "\n",
    "# Define the output path\n",
    "output_path = os.path.join(OUTPUT_DATA_DIR, f\"{name}.parquet\")\n",
    "\n",
    "# Save the cleaned dataframe to a new Parquet file\n",
    "df.to_parquet(output_path, index=False)\n",
    "print(f\"Successfully saved cleaned data to: {output_path}\")\n",
    "\n",
    "print(\"\\nData cleaning process finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
