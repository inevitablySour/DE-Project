{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c4d3c8",
   "metadata": {},
   "source": [
    "# Data engineering Healthcare Project\n",
    "---\n",
    "### Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f9a49",
   "metadata": {},
   "source": [
    "In this notebook, we analyze the provided datasets and compare three database systems.\n",
    "We also motivate our final database choice based on the structure and properties of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f9b06",
   "metadata": {},
   "source": [
    "### 1. Importing Our Tools (Libraries) & Loading Data\n",
    "We will use pandas for data manipulation, and matplotlib and seaborn for creating charts. This is the standard and most effective toolkit for this kind of analysis in Python. After importing, we load our six pre-cleaned CSV files into pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff4118",
   "metadata": {},
   "source": [
    "* import pandas as pd: We use pandas to work with data tables (like spreadsheets). It lets us load our data from CSV files into something called a DataFrame. Think of a DataFrame as a smart table where we can easily sort, filter, and work with our data.\n",
    "* Why pandas? Why not just use Python's built-in lists or dictionaries? While we could use basic Python to handle the data, it would be much harder. We would have to write a lot more code to do simple things like calculating the average of a column. pandas is the standard tool for data analysis in Python because it's powerful and easy to use for table-like data.\n",
    "* import numpy as np: numpy is a library for working with numbers and math. pandas actually uses numpy behind the scenes. We import it directly because it's a good habit and sometimes we need it for special math operations.\n",
    "* import matplotlib.pyplot as plt and import seaborn as sns: These two libraries are used for making charts and graphs (visualizations). Pictures help us understand the data much better than just looking at numbers.\n",
    "* Why use two plotting libraries? matplotlib is the original, foundational plotting library. It can make almost any chart, but it can sometimes take a lot of code to make it look good. seaborn is built on top of matplotlib and makes it much easier to create beautiful and common statistical plots (like bar charts and heatmaps) with less code. We use them together to get the best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7193dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a757a5",
   "metadata": {},
   "source": [
    "### 2. Loading the Data Files\n",
    "Now we use the pandas library, which we called pd, to load our six CSV files. Each file contains a different piece of the healthcare puzzle. We give each one a unique name so we can work with it separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f82b17",
   "metadata": {},
   "source": [
    "Our teammate already cleaned the data a little bit with general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aa1b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = pd.read_csv(\"../cleaned_data/observations_cleaned.csv\")\n",
    "patients = pd.read_csv(\"../cleaned_data/patients_cleaned.csv\")\n",
    "procedures = pd.read_csv(\"../cleaned_data/procedures_cleaned.csv\")\n",
    "\n",
    "diagnoses = pd.read_csv(\"../cleaned_data/diagnoses_cleaned.csv\")\n",
    "encounters = pd.read_csv('../cleaned_data/encounters_cleaned.csv')\n",
    "medications = pd.read_csv('../cleaned_data/medications_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb68aa",
   "metadata": {},
   "source": [
    "### 3. Making a Smart Plan: The Reusable Analysis Function\n",
    "We have six different data files to check. We could copy and paste our analysis code six times, but that's a bad idea.\n",
    "* The wrong way (the alternative): If we copied and pasted the code, our notebook would be very long and repetitive. Even worse, if we found a mistake or wanted to add a new chart, we would have to fix it in all six places! It's easy to make a mistake this way.\n",
    "* The right way (our implementation): We create a single, reusable function called analyze_dataframe. Think of it like a recipe. We write the steps for our analysis once inside this function. Then, for each data file, we just \"call\" the function. This is cleaner, safer, and much easier to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05979c",
   "metadata": {},
   "source": [
    "#### What's Inside Our \"Recipe\"?\n",
    "Our function will automatically perform a standard check-up on any DataFrame we give it:\n",
    "1. .shape, .head(), .info(): These commands give us a quick first look. We see how many rows and columns there are, what the first few rows look like, and what kind of data is in each column (e.g., numbers, text, dates).\n",
    "2. .isnull().sum(): This checks for missing data. Missing values can cause errors, so we need to know where they are.\n",
    "3. .describe(include='all'): This gives us statistics. For number columns, it shows the average, min, max, etc. By adding include='all', we tell it to also give us information about text columns, like how many unique text values there are.\n",
    "4. Visualizations (Charts): We create a few standard charts to help us \"see\" the data.\n",
    "* Histograms for numbers: This is the best way to see the distribution or \"shape\" of numerical data. It shows us if most values are low, high, or spread out evenly.\n",
    "* Bar charts for text: This is perfect for counting how many times each item in a category appears.\n",
    "* Why not a pie chart? Pie charts can be hard to read if there are more than two or three categories. It's difficult for our eyes to compare the size of slices. Bar charts are almost always easier to understand and compare.\n",
    "* Safety Check (if df[col].nunique() < 20): We only create bar charts for text columns with fewer than 20 unique categories. Why? Imagine trying to create a bar chart for a \"patient_name\" column with 1000 names. It would be a giant, unreadable mess. This check prevents that and keeps our analysis useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframe(df, filename):\n",
    "    \"\"\"\n",
    "    Performs and prints a full analysis for a given DataFrame,\n",
    "    including data visualizations.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ANALYSIS FOR: {filename}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"\\n[INFO] Shape of the dataset (rows, columns): {df.shape}\\n\")\n",
    "    \n",
    "    # Head\n",
    "    print(\"[INFO] First 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "    \n",
    "    # Info\n",
    "    print(\"[INFO] Technical Information (Data Types and Non-Null Counts):\")\n",
    "    df.info()\n",
    "    print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "\n",
    "    # Missing Values\n",
    "    print(\"[INFO] Count of Missing (Null) Values per Column:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() == 0:\n",
    "        print(\"No missing values found.\")\n",
    "    else:\n",
    "        print(missing_values[missing_values > 0])\n",
    "    print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "\n",
    "    # Descriptive Statistics\n",
    "    print(\"[INFO] Descriptive Statistics for All Columns:\")\n",
    "    print(df.describe(include='all'))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    # VISUALIZATIONS\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"VISUALIZATIONS FOR: {filename}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Missing Value Heatmap\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "        plt.title(f'Missing Value Heatmap for {filename}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Histograms for Numerical Columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\n[VISUAL] Histograms for Numerical Columns in {filename}:\")\n",
    "        df[numeric_cols].hist(bins=15, figsize=(15, 10), layout=(-1, 3))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Count Plots for Categorical Columns\n",
    "    # We only plot categorical columns with a reasonable number of unique values (< 20)\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\n[VISUAL] Count Plots for Key Categorical Columns in {filename}:\")\n",
    "        for col in categorical_cols:\n",
    "            if df[col].nunique() < 20:\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                sns.countplot(y=col, data=df, order=df[col].value_counts().index)\n",
    "                plt.title(f'Count of each category in: {col}')\n",
    "                plt.xlabel('Count')\n",
    "                plt.ylabel(col)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342766da",
   "metadata": {},
   "source": [
    " ---\n",
    "### 2. Dataset-Specific Analysis\n",
    "Patients Dataset\n",
    "Summary: This is the master table for patient demographic data. It contains Personally Identifiable Information (PII) like names, addresses, and birth dates, which is a major consideration for data security (GDPR). The data is high quality with no missing values. The date_of_birth column is stored as a string (object) and will require transformation into a proper date type for age calculations. The patient_id column serves as the primary key to link to all other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bf33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(observations, 'Oberservations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f021968",
   "metadata": {},
   "source": [
    "#### Deeper Analysis & Commentary: Observations\n",
    "* Structure and Keys: With 886 rows and 9 columns, this is a \"tall\" table, which is typical for transactional or event-based data like lab results. The observation_id column is unique for every single row, which makes it the Primary Key for this table. This means we can use this ID to refer to a specific, unique observation. patient_id and encounter_id are Foreign Keys, linking each observation back to a patient and a specific visit.\n",
    "* Data Types - A Critical Finding: The observation_datetime column is stored as an object (text), not a proper date type. This is a crucial finding. For our database, we must store this as a DATETIME or TIMESTAMP type. This allows for correct sorting, filtering by date ranges, and calculating time differences, which is impossible with text.\n",
    "* Missing Value Pattern: This is very insightful. units and value_numeric are both missing 25 values. Meanwhile, value_text contains exactly 25 non-null values. This tells a clear story: an observation is either a number (with units) OR it's a text value (like \"Positive\" or \"Negative\"). This is actually a good, clean data structure. Our database design should probably have a value_numeric column and a value_text column to reflect this.\n",
    "* Content Insights:\n",
    "* The dataset contains results for only 9 unique types of observations. The most common are lab tests like 'Creatinine' and 'Hemoglobin A1c', as well as blood pressure readings.\n",
    "* These observations belong to 150 unique patients, which matches the total number of patients in our patients table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(patients, 'Patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ac493",
   "metadata": {},
   "source": [
    "#### Deeper Analysis & Commentary: Patients\n",
    "* Structure and Keys: This is our central \"master\" table for patients. With 150 rows and 10 columns, each row represents one unique person. The patient_id column has 150 unique values, confirming it is the Primary Key. All other tables will link to this one using patient_id.\n",
    "* Data Quality: The data quality is excellent, with no missing values. This is great, but in a real-world scenario, this is rare.\n",
    "* Data Types - Important for Engineering:\n",
    "* Just like in the previous table, date_of_birth is an object (text). This MUST be converted to a DATE type in our final database. This is essential for correctly calculating patient ages.\n",
    "* zip_code is stored as an int64 (a number). This is generally acceptable, but it's often better practice to store codes like this as text (VARCHAR in a database). This prevents issues like dropping leading zeros (e.g., '07039' becoming '7039') and makes it clear that we shouldn't perform mathematical operations (like calculating the average) on them.\n",
    "* Content Insights:\n",
    "* The data contains sensitive, Personally Identifiable Information (PII) like names, addresses, and phone numbers. This has huge implications for our data engineering. The database and any applications built on it must have strong security measures, including encryption and access controls, to comply with privacy laws like GDPR and HIPAA.\n",
    "* The gender split is almost 50/50 (79 Male vs. 71 Female).\n",
    "* The most common last name is 'Smith', which is a common feature of synthetically generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00068ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(procedures, 'Procedures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810208f0",
   "metadata": {},
   "source": [
    "#### Deeper Analysis & Commentary: Procedures\n",
    "* Structure and Scope: This is a very small table with only 54 records. This tells us that either procedures are rare events for this patient group, or the dataset is limited in scope.\n",
    "* Data Types: Once again, date_performed is an object (text) and must be converted to a DATE or DATETIME type in the database.\n",
    "* Content Insights:\n",
    "* The most striking observation is that there are only 2 unique procedures in the entire dataset: \"Influenza virus vaccine, quadrivalent\" and \"Ophthalmological examination...\". This is a very limited set. This means we cannot use this data to analyze a wide range of surgical or medical procedures.\n",
    "* 50 unique patients are represented, with one patient having had 2 procedures.\n",
    "* The procedure_id is the Primary Key for this table as it's unique for every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(diagnoses, 'Diagnoses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4941f1c",
   "metadata": {},
   "source": [
    "#### Deeper Analysis & Commentary: Diagnoses\n",
    "* Structure: With 221 records for 150 patients, it's clear that patients can have more than one diagnosis, which is expected.\n",
    "* Data Types: date_recorded is an object and should be converted to a DATE type.\n",
    "* Content Insights:\n",
    "* Similar to procedures, the scope is very limited. There are only 5 unique diagnoses.\n",
    "* The most common diagnosis is 'Essential (primary) hypertension' (high blood pressure), appearing 65 times. The next most common is 'Type 2 diabetes mellitus'. This gives us a very clear picture of the primary health concerns of this patient population. This aligns with the 'Metformin' medication we might see later.\n",
    "* diagnosis_id is the Primary Key. encounter_id is also unique for every row, suggesting that in this dataset, only one diagnosis is recorded per encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ddffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(encounters, 'Encounters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f511249",
   "metadata": {},
   "source": [
    "#### Deeper Analysis & Commentary: Encounters\n",
    "* Structure: This table acts as the \"glue,\" with 291 unique visits linking patients to physicians and other events.\n",
    "* Data Types: admission_date and discharge_date are both text (object) and need to be converted to DATETIME types. This would allow us to calculate the duration of a visit.\n",
    "* The Most Important Finding: The visit_type column is the most critical insight from this table. It has only one unique value: 'Outpatient'. This is a major constraint on our dataset. It means we have no data on hospitalizations, emergency room visits, or telehealth appointments. Any analysis or system we build must acknowledge that it is based purely on outpatient data.\n",
    "* Content Insights:\n",
    "* The dataset covers all 150 patients.\n",
    "* There are 108 unique attending_physician_ids, indicating a fairly large number of doctors are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51455f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe(medications, 'Medications')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d4eec",
   "metadata": {},
   "source": [
    "#### Deeper Analysis & Commentary: Medications\n",
    "* Structure: 160 medication orders for 126 unique patients. This means not all patients in the dataset are on medication.\n",
    "* Missing Values - A Key Story:\n",
    "* end_date is missing for 120 of the 160 records (75%). This is not a data quality problem; it's a feature. It tells us that these are likely active, ongoing prescriptions. A null end_date means the patient is still supposed to be taking the medication. This is a critical business rule to build into our database logic.\n",
    "* dosage is missing for 15 records. This, unlike the end_date, is likely a data quality issue that would need to be addressed in a real project.\n",
    "* Data Types: start_date and end_date are text (object) and must be converted to DATE types.\n",
    "* Content Insights:\n",
    "* There are only 5 unique drugs. The most common is 'Metformin HCl 500mg tablet', a drug for diabetes. This, combined with our findings from the diagnoses table, confirms that diabetes and hypertension are the primary conditions managed in this dataset.\n",
    "* All medications are taken 'Oral'-ly. There are no injections or other routes of administration. This further simplifies the nature of our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
